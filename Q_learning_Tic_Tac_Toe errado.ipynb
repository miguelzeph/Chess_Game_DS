{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:00.797533Z",
     "start_time": "2021-04-18T11:08:00.362610Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:00.825747Z",
     "start_time": "2021-04-18T11:08:00.799960Z"
    },
    "code_folding": [
     25,
     52,
     55,
     59,
     107,
     121
    ]
   },
   "outputs": [],
   "source": [
    "# Enviroment\n",
    "class Enviroment(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "\n",
    "        # Board (é nosso ESTADO ATUAL)\n",
    "        self.board = np.zeros((3,3))\n",
    "\n",
    "        # Set of States\n",
    "        self.states = []\n",
    "        \n",
    "        # pos jogada\n",
    "        self.pos = 0\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.board = np.zeros((3,3))\n",
    "\n",
    "    # Plotar o Board\n",
    "    def draw_board(self):\n",
    "\n",
    "        draw = ''\n",
    "\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                simbolo = ''\n",
    "                # simbolo X (p1 = 1) ou O (p2 = -1)\n",
    "                if self.board[i][j] == 1:\n",
    "                    symbol = 'X'\n",
    "                elif self.board[i][j] == -1:\n",
    "                    symbol = 'O'\n",
    "                else:\n",
    "                    symbol = ' '\n",
    "\n",
    "\n",
    "                draw += '|'+symbol+''\n",
    "\n",
    "\n",
    "\n",
    "                if j == 2:\n",
    "\n",
    "                    draw +='|\\n-------\\n'\n",
    "\n",
    "        print(draw)\n",
    "\n",
    "    # Posições disponíveis\n",
    "    def available_moves(self):\n",
    "        return np.argwhere(self.board == 0)\n",
    "    # Jogar uma posição disponível\n",
    "    def available_move_choice(self):\n",
    "        return random.choice(self.available_moves())\n",
    "\n",
    "    # Checar Resultado    \n",
    "    def check_result(self):\n",
    "\n",
    "        # Row\n",
    "        if sum(self.board[0]) == 3 or sum(self.board[1]) == 3 or sum(self.board[2]) == 3:\n",
    "            #print('venceu')\n",
    "            return 1\n",
    "        if sum(self.board[0]) == -3 or sum(self.board[1]) == -3 or sum(self.board[2]) == -3:\n",
    "            #print('perdeu')\n",
    "            return -1\n",
    "        # Col\n",
    "        if sum(self.board[:,0]) == 3 or sum(self.board[:,1]) == 3 or sum(self.board[:,2]) == 3:\n",
    "            #print('venceu')\n",
    "            return 1\n",
    "        if sum(self.board[:,0]) == - 3 or sum(self.board[:,1]) == - 3 or sum(self.board[:,2]) == - 3:\n",
    "            #print('perdeu')\n",
    "            return -1\n",
    "        # Diagonal\n",
    "        if sum(self.board.diagonal()) == 3 or sum(np.fliplr(self.board).diagonal()) == 3:\n",
    "            #print('venceu')\n",
    "            return 1\n",
    "        if sum(self.board.diagonal()) == -3 or sum(np.fliplr(self.board).diagonal()) == -3:\n",
    "            #print('perdeu')\n",
    "            return -1\n",
    "        # Empate\n",
    "        if not 0 in self.board:\n",
    "            #print('empate')\n",
    "            return 0\n",
    "\n",
    "        #########################################################\n",
    "        ## continua = 2, empate = 0, vitoria = 1, derrota = -1 ##\n",
    "        #########################################################\n",
    "\n",
    "        return 2\n",
    "\n",
    "    # Dar recompensa        \n",
    "    def reward(self, result):\n",
    "\n",
    "        if result == 1:  # Vitória\n",
    "            return 1\n",
    "\n",
    "        if result == -1: # Derrota\n",
    "            return -1\n",
    "\n",
    "        if result == 0:  # Empate\n",
    "            return 0.001\n",
    "    \n",
    "    \n",
    "    # jogada - Random \n",
    "    def select_pos_by_random(self, player, name):\n",
    "        \n",
    "        row_col = self.available_move_choice()\n",
    "        \n",
    "        row = row_col[0] # Linha\n",
    "        col = row_col[1] # Coluna\n",
    "\n",
    "        self.board[row][col] = player\n",
    "        \n",
    "        self.pos = row,col\n",
    "        \n",
    "        #print(name + f' jogou na posição { str(self.pos) }')\n",
    "           \n",
    "    # jogada - humano   \n",
    "    def select_pos_by_input(self, player, name):\n",
    "        \n",
    "        # desenhar jogada do player \n",
    "        self.draw_board()\n",
    "        while True:\n",
    "            row = int( input('Row: ') )\n",
    "            col = int( input('Col: ') )\n",
    "            \n",
    "            if [row,col] in self.available_moves().tolist(): # Refransforme Em lista... Array ele aceita \n",
    "                \n",
    "                self.board[row][col] = player\n",
    "                self.pos = row,col\n",
    "                break\n",
    "            else:\n",
    "                input('try other position...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:00.976493Z",
     "start_time": "2021-04-18T11:08:00.828185Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Agent\n",
    "class Agent(Enviroment):\n",
    "\n",
    "    def __init__(self, lr, gamma, epsilon):\n",
    "        \n",
    "        \n",
    "        super().__init__( ) # Precisava Herdar alguns métodos do Enviroment\n",
    "\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.player = 1 # player 1 = 1 and player 2 = -1\n",
    "        \n",
    "        self.number_match = 0\n",
    "        \n",
    "        self.results ={\n",
    "            'win':0,\n",
    "            'draw':0,\n",
    "            'lost':0}\n",
    "        \n",
    "        self.Q_table = {\n",
    "            'states' : [],\n",
    "            'actions': ['(0, 0)','(0, 1)','(0, 2)','(1, 0)','(1, 1)','(1, 2)','(2, 0)','(2, 1)','(2, 2)'],\n",
    "            'Q': []\n",
    "        }\n",
    "        \n",
    "        self.path = {\n",
    "            'states':  [], # boards\n",
    "            'actions': [], # posição no tabuleiro\n",
    "        }\n",
    "        \n",
    "    def reset_game(self):\n",
    "        self.player = 1\n",
    "        self.path = {\n",
    "            'states': [],\n",
    "            'actions':[],\n",
    "        }\n",
    "    \n",
    "    def reset_historic_game(self):\n",
    "        self.results ={\n",
    "            'win':0,\n",
    "            'draw':0,\n",
    "            'lost':0}\n",
    "        \n",
    "    def save_result(self, resultado):\n",
    "        \n",
    "        if resultado == 1:\n",
    "            #print('venceu')\n",
    "            self.results['win'] += 1\n",
    "            \n",
    "        elif resultado == -1:\n",
    "            #print('perdeu')\n",
    "            self.results['lost'] += 1\n",
    "\n",
    "        else:\n",
    "            #print('empate')\n",
    "            self.results['draw'] += 1\n",
    "            \n",
    "    def Q_table_df(self):\n",
    "        \n",
    "        df = pd.DataFrame(\n",
    "            index= self.Q_table['states'],\n",
    "            columns= self.Q_table['actions'],\n",
    "            data = self.Q_table['Q']\n",
    "            )\n",
    "            #data = 0 )\n",
    "        return df\n",
    "    \n",
    "    def update_Q(self, reward):\n",
    "        \n",
    "        # Q(s,a) = Q(s,a) + alpha* ( R(s) + * Gamma * max_Q(s+1,:) - Q(s,a) ) )\n",
    "        # R(s) = Reward...\n",
    "        \n",
    "        lr =    self.lr    #0.9 # Alpha - Taxa de Aprendizagem\n",
    "        gamma = self.gamma #0.9 # Gamma - Fator de Desconto\n",
    "        \n",
    "        \n",
    "        # Lista de Estados e Ações - Executados\n",
    "        states_actions = list( self.path.values() )\n",
    "\n",
    "        # Lista de Estados Reverso (pois iremos do FUTURO pro PASSADO)\n",
    "        states =  list( reversed( states_actions[0] ) )\n",
    "\n",
    "        # Lista de Ações Reverso   (pois iremos do FUTURO pro PASSADO)\n",
    "        actions = list( reversed( states_actions[1] ) )\n",
    "\n",
    "        # Marcador para eu saber onde estou\n",
    "        index = 0\n",
    "        for s2, a2 in zip( states, actions ):\n",
    "            \n",
    "            \n",
    "            if reward >= 0: \n",
    "\n",
    "                try:\n",
    "                    # index  = 0 é a ultima ação que levou a vitóriam, ou derrota\n",
    "                    if index == 0:\n",
    "\n",
    "                        s2 = self.Q_table['states'].index(str(s2))\n",
    "                        a2 = self.Q_table['actions'].index(str(a2))\n",
    "\n",
    "                        self.Q_table['Q'][s2][a2] = lr* ( reward ) #self.Q_table['Q'][s2][a2] = reward \n",
    "\n",
    "\n",
    "                        # Fazer o mesmo, mas agora para o States adiantado\n",
    "\n",
    "                        ##### Next Value #####\n",
    "\n",
    "                        # ESTADO avançado\n",
    "                        index += 1\n",
    "\n",
    "                        s1 = states[index]\n",
    "                        s1 = self.Q_table['states'].index(str(s1))\n",
    "\n",
    "                        a1 = actions[index]\n",
    "                        a1 = self.Q_table['actions'].index(str(a1))\n",
    "\n",
    "                        # a2 -> deixa em aberto, por que estamos interessado na ação com valor MÁXIMO do respectivo ESTADO avançado Max_Q(s+1,:)\n",
    "                        self.Q_table['Q'][s1][a1] += lr*( 0 + gamma*np.max( self.Q_table['Q'][s2] ) - self.Q_table['Q'][s1][a1] )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ##### pegar o index numérico dos States e Actions\n",
    "                        s2 = self.Q_table['states'].index(str(s2))\n",
    "                        a2 = self.Q_table['actions'].index(str(a2))\n",
    "\n",
    "\n",
    "                        # Fazer o mesmo, mas agora para o States adiantado\n",
    "\n",
    "                        ##### Next Value #####\n",
    "\n",
    "                        # ESTADO avançado\n",
    "                        index += 1\n",
    "\n",
    "                        s1 = states[index]\n",
    "                        s1 = self.Q_table['states'].index(str(s1))\n",
    "\n",
    "                        a1 = actions[index]\n",
    "                        a1 = self.Q_table['actions'].index(str(a1))\n",
    "\n",
    "                        # a2 -> deixa em aberto, por que estamos interessado na ação com valor MÁXIMO do respectivo ESTADO avançado Max_Q(s+1,:)\n",
    "                        self.Q_table['Q'][s1][a1] += lr*( 0 + gamma*np.max( self.Q_table['Q'][s2] ) - self.Q_table['Q'][s1][a1] ) \n",
    "\n",
    "                # Não há mais Estados Adiantados para buscar.   \n",
    "                except IndexError:\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            if reward < 0:\n",
    "            # Se for negativo tem que DESCONTAR, pra isso, usa-se o MIN_Q\n",
    "                \n",
    "                try:\n",
    "                    # index  = 0 é a ultima ação que levou a vitóriam, ou derrota\n",
    "                    if index == 0:\n",
    "\n",
    "                        s2 = self.Q_table['states'].index(str(s2))\n",
    "                        a2 = self.Q_table['actions'].index(str(a2))\n",
    "\n",
    "                        self.Q_table['Q'][s2][a2] = lr* ( reward ) #self.Q_table['Q'][s2][a2] = reward \n",
    "\n",
    "\n",
    "                        # Fazer o mesmo, mas agora para o States adiantado\n",
    "\n",
    "                        ##### Next Value #####\n",
    "\n",
    "                        # ESTADO avançado\n",
    "                        index += 1\n",
    "\n",
    "                        s1 = states[index]\n",
    "                        s1 = self.Q_table['states'].index(str(s1))\n",
    "\n",
    "                        a1 = actions[index]\n",
    "                        a1 = self.Q_table['actions'].index(str(a1))\n",
    "\n",
    "                        # a2 -> deixa em aberto, por que estamos interessado na ação com valor MÁXIMO do respectivo ESTADO avançado Max_Q(s+1,:)\n",
    "                        self.Q_table['Q'][s1][a1] += lr*( 0 + gamma*np.min( self.Q_table['Q'][s2] ) - self.Q_table['Q'][s1][a1] )\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ##### pegar o index numérico dos States e Actions\n",
    "                        s2 = self.Q_table['states'].index(str(s2))\n",
    "                        a2 = self.Q_table['actions'].index(str(a2))\n",
    "\n",
    "\n",
    "                        # Fazer o mesmo, mas agora para o States adiantado\n",
    "\n",
    "                        ##### Next Value #####\n",
    "\n",
    "                        # ESTADO avançado\n",
    "                        index += 1\n",
    "\n",
    "                        s1 = states[index]\n",
    "                        s1 = self.Q_table['states'].index(str(s1))\n",
    "                        \n",
    "                        \n",
    "\n",
    "                        a1 = actions[index]\n",
    "                        a1 = self.Q_table['actions'].index(str(a1))\n",
    "                        \n",
    "                        print(a1)\n",
    "                        input()\n",
    "\n",
    "                        # a2 -> deixa em aberto, por que estamos interessado na ação com valor MÁXIMO do respectivo ESTADO avançado Max_Q(s+1,:)\n",
    "                        self.Q_table['Q'][s1][a1] += lr*( 0 + gamma*np.min( self.Q_table['Q'][s2] ) - self.Q_table['Q'][s1][a1] ) \n",
    "\n",
    "                # Não há mais Estados Adiantados para buscar.   \n",
    "                except IndexError:\n",
    "                    continue   \n",
    "    \n",
    "    \n",
    "    \n",
    "    def select_pos_by_Q( self, player, name):\n",
    "\n",
    "        # Veja o estado atual seu (Seu board)... pegue a ação com maior Q\n",
    "        \n",
    "        \n",
    "        # jogada Aleatória\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            \n",
    "            #print('********jogada aleatória - Caiu no EPSILON ***********')\n",
    "            \n",
    "            self.select_pos_by_random( self.player, name = 'player '+str(self.player) )\n",
    "\n",
    "\n",
    "            #print('usando aleatório')\n",
    "\n",
    "        # Vai na tabela e joga\n",
    "        else:\n",
    "\n",
    "            # Se existir esse estado gravado...\n",
    "\n",
    "            if str(self.board) in self.Q_table['states']:\n",
    "\n",
    "\n",
    "                #print('usando o Q')\n",
    "\n",
    "\n",
    "                index_state = self.Q_table['states'].index( str(self.board) )\n",
    "                #index_action= self.Q_table['Q'][index_state].index( str(np.max(self.Q_table['Q'][index_state])) )\n",
    "                #index_qmax = np.argmax(self.Q_table['Q'][index_state])\n",
    "\n",
    "\n",
    "                # pega todos valores de Q com respectivo index state na ordem DESCRESCENTE\n",
    "                # assim, se a posição máx já estiver ocupada, ele vai pro segundo maior e assim por diante.\n",
    "\n",
    "                #print(sorted( self.Q_table['Q'][index_state], reverse = True ) )\n",
    "                #input()\n",
    "                \n",
    "\n",
    "                # pega o maior na ordem decrescente... \n",
    "                for qmax in sorted( self.Q_table['Q'][index_state], reverse = True ):\n",
    "                    \n",
    "                    # logo se for Zero não temos estado treinado\n",
    "                    if qmax == 0:\n",
    "                        \n",
    "                        #print(f'********Jogada Aleatório - qmax = {qmax} ... não tem treino***********')\n",
    "                        \n",
    "                        self.select_pos_by_random( self.player, name = 'player '+str(self.player) )\n",
    "                        break\n",
    "\n",
    "\n",
    "                    index_qmax = self.Q_table['Q'][index_state].index( qmax )\n",
    "\n",
    "                    action = self.Q_table['actions'][index_qmax]\n",
    "\n",
    "                    row = int(action[1:2])\n",
    "                    col = int(action[4:5])\n",
    "\n",
    "                    if [row,col] in self.available_moves().tolist(): # Refransforme Em lista... Array ele aceita  \n",
    "\n",
    "                        self.board[row][col] = player\n",
    "\n",
    "                        self.pos = row,col\n",
    "                        \n",
    "                        #print(f'******** Jogada Inteligente - melhor Q:{qmax}***********')\n",
    "\n",
    "\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "            # se não existir, joga aleatório mesmo\n",
    "            else:\n",
    "                \n",
    "                #print('********Jogada Aleatória - Não existe este Estado***********')\n",
    "                \n",
    "                #print(str(self.board))\n",
    "                \n",
    "                self.select_pos_by_random( self.player, name = 'player '+str(self.player) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.096253Z",
     "start_time": "2021-04-18T11:08:00.979370Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# funct to start the game\n",
    "def start():\n",
    "    while True:\n",
    "\n",
    "        ##################### Criação da Tabela Q (antes) ###################\n",
    "        # Se não existe este Estado dentro da Tabela Q, adicione\n",
    "        if str(agent.board) not in agent.Q_table['states']:\n",
    "\n",
    "            # 1-) Adicionar Estado Atual\n",
    "            agent.Q_table['states'].append( str(agent.board ) )\n",
    "\n",
    "            # 2-) Add valor de Q\n",
    "            agent.Q_table['Q'].append( [0,0,0,0,0,0,0,0,0] )\n",
    "        ###############################################################\n",
    "\n",
    "\n",
    "        # Registrar o State Inicial no PATH\n",
    "        agent.path['states'].append( str(agent.board) )\n",
    "\n",
    "\n",
    "        ############################ Agente Executa Ação no Ambiente #################### \n",
    "        if agent.player == 1: # PLAYER 1\n",
    "            #env.select_pos_by_random( agent.player, name = 'player '+str(agent.player) )\n",
    "            agent.select_pos_by_Q( agent.player, name = 'player '+str(agent.player) )\n",
    "\n",
    "        else:               # PLAYER 2 \n",
    "            agent.select_pos_by_random( agent.player, name = 'player '+str(agent.player) )\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "        # Registrar o Action realizada no PATH\n",
    "        agent.path['actions'].append( str(agent.pos) )\n",
    "\n",
    "\n",
    "\n",
    "        # ( Desenha  Board )\n",
    "        #env.draw_board()\n",
    "\n",
    "        ########################## Ambiente Responde ######################################\n",
    "        # checa resultado\n",
    "        if agent.check_result() != 2: # continua = 2, empate = 0, vitoria = 1, derrota = -1\n",
    "\n",
    "            # resultado do jogo\n",
    "            agent.save_result( agent.check_result() )\n",
    "\n",
    "            # Valor da Recompensa\n",
    "            reward = agent.reward( agent.check_result() )\n",
    "\n",
    "            # Update Q Table\n",
    "            agent.update_Q( reward )\n",
    "\n",
    "            # Reset Game\n",
    "            agent.reset_game()\n",
    "            agent.reset_game()\n",
    "            \n",
    "            # add partida jogada\n",
    "            agent.number_match += 1\n",
    "\n",
    "            break\n",
    "            \n",
    "\n",
    "\n",
    "        # Mudar jogador    \n",
    "        agent.player *= -1 # switch players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.208533Z",
     "start_time": "2021-04-18T11:08:01.098581Z"
    }
   },
   "outputs": [],
   "source": [
    "# SALVAR \n",
    "def save_Q_table():\n",
    "    with open(\"./trained/Q_table.pkl\", \"wb\") as tf:\n",
    "        pickle.dump(agent.Q_table,tf)\n",
    "\n",
    "    with open(\"./trained/partidas.pkl\", \"wb\") as tf:\n",
    "        pickle.dump(agent.number_match,tf)\n",
    "\n",
    "# LOAD\n",
    "def load_Q_table():\n",
    "    #### Fazer Load dos dados já treinados ####\n",
    "    with open('./trained/Q_table.pkl', 'rb') as handle:\n",
    "        Q_table = pickle.load(handle)\n",
    "    with open('./trained/partidas.pkl', 'rb') as handle:\n",
    "        number_match = pickle.load(handle)\n",
    "\n",
    "    agent.number_match = number_match\n",
    "    agent.Q_table = Q_table \n",
    "\n",
    "    print(f\"número de partidas {agent.number_match}\")\n",
    "    #############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.319190Z",
     "start_time": "2021-04-18T11:08:01.214999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Objet Player\n",
    "agent = Agent( lr = 0.9, gamma = 0.9, epsilon = 0.0  )\n",
    "\n",
    "# Load Q Table\n",
    "#load_Q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.403772Z",
     "start_time": "2021-04-18T11:08:01.322388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Object Enviroment\n",
    "env = Enviroment( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.811877Z",
     "start_time": "2021-04-18T11:08:01.407040Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Cannot choose from an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6cc84d73c611>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#  Treina 1000 x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" partidas jogadas: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_match\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_historic_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1c71279b4c0b>\u001b[0m in \u001b[0;36mstart\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# PLAYER 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#env.select_pos_by_random( agent.player, name = 'player '+str(agent.player) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_pos_by_Q\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'player '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m               \u001b[0;31m# PLAYER 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c704f1f8d80a>\u001b[0m in \u001b[0;36mselect_pos_by_Q\u001b[0;34m(self, player, name)\u001b[0m\n\u001b[1;32m    260\u001b[0m                         \u001b[0;31m#print(f'********Jogada Aleatório - qmax = {qmax} ... não tem treino***********')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_pos_by_random\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'player '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-63de6e1d89b7>\u001b[0m in \u001b[0;36mselect_pos_by_random\u001b[0;34m(self, player, name)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_pos_by_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mrow_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_move_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_col\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Linha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-63de6e1d89b7>\u001b[0m in \u001b[0;36mavailable_move_choice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Jogar uma posição disponível\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mavailable_move_choice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavailable_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Checar Resultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
     ]
    }
   ],
   "source": [
    "# Cada época\n",
    "for k in range(100):\n",
    "    \n",
    "    #  Treina 1000 x\n",
    "    for i in range(100):\n",
    "        start()\n",
    "    print(str(agent.results) + \" partidas jogadas: \" + str(agent.number_match) )\n",
    "    agent.reset_historic_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.817249Z",
     "start_time": "2021-04-18T11:08:00.396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Q_TABLE ---> States X Actions\n",
    "agent.Q_table_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T11:08:01.819539Z",
     "start_time": "2021-04-18T11:08:00.399Z"
    }
   },
   "outputs": [],
   "source": [
    "# SAVE Q Table\n",
    "#save_Q_table()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
